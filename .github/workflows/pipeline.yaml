name: Scrape News and Submit to Database

on:
  schedule:
    - cron: "0 3 * * *" # Runs at 10:00 AM UTC+7 every day
  workflow_dispatch:

jobs:
  run_scraper_and_submit:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository content
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Debug installed packages
        run: pip list

      - name: Run scraping script for all news source
        run: python scripts/pipeline.py 1 pipeline
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DB_KEY: ${{ secrets.DB_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          proxy: ${{ secrets.PROXY }}

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add -A
          git diff-index --quiet HEAD || (git commit -m "Updated scraped news articles" --allow-empty)
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
